{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c983e7-a4f2-4393-8a1d-030dbdf311e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb001fd-ce6f-4946-bb2a-5234fe3c2640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    # transformations for the input data\n",
    "    transforms = Compose([\n",
    "        ToTensor(),\n",
    "        Resize(224),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # read input image\n",
    "    input_img = cv2.imread(img_path)\n",
    "    # do transformations\n",
    "    input_data = transforms(input_img)\n",
    "    batch_data = torch.unsqueeze(input_data, 0)\n",
    "    return batch_data\n",
    "\n",
    "def postprocess(output_data):\n",
    "    # get class names\n",
    "    with open(\"../data/imagenet_classes.txt\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    # calculate human-readable value by softmax\n",
    "    confidences = torch.nn.functional.softmax(output_data, dim=1)[0] * 100\n",
    "    # find top predicted classes\n",
    "    _, indices = torch.sort(output_data, descending=True)\n",
    "    i = 0\n",
    "    # print the top classes predicted by the model\n",
    "    while confidences[indices[0][i]] > 0.5:\n",
    "        class_idx = indices[0][i]\n",
    "        print(\n",
    "            \"class:\",\n",
    "            classes[class_idx],\n",
    "            \", confidence:\",\n",
    "            confidences[class_idx].item(),\n",
    "            \"%, index:\",\n",
    "            class_idx.item(),\n",
    "        )\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311dde19",
   "metadata": {},
   "source": [
    "## Step 1 : Just Torch based inference\n",
    "Here, we use a pretrained Resnet50 model to classify an input image. The inference in done purely in Pytorch. The model's prediction is passed through a post processing engine to get final predction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3358a98e-68c2-4bd5-bcec-6e3fd0e04c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: cup , confidence: 94.97858428955078 %, index: 968\n",
      "class: espresso , confidence: 3.9512522220611572 %, index: 967\n",
      "class: coffee mug , confidence: 0.6196928024291992 %, index: 504\n"
     ]
    }
   ],
   "source": [
    "input = preprocess_image(\"../data/turkish_coffee.jpg\").cuda()\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "output = model(input)\n",
    "\n",
    "postprocess(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68601ac",
   "metadata": {},
   "source": [
    "## Step 2 : Convert Model to ONNX\n",
    "Here, we first convert the given model to ONNX representation which will be later converted to TensorRT engine. There are several ways tto convert a model to TensorRT, but the most common method is using ONNX representation. We pass dummy input of the same shape as expected inputs, the model instance to the export function to convert a given torch network to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "387ab26d-4642-494a-bd3a-0f2349baa471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ONNX_FILE_PATH = '../onnx_files/resnet50.onnx'\n",
    "torch.onnx.export(model, input, ONNX_FILE_PATH, input_names=['input'],\n",
    "                  output_names=['output'], export_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd3e63",
   "metadata": {},
   "source": [
    "## Step 3: Convert ONNX model to TensorRT engine\n",
    "Now, we finally convert this generated ONNX model to TensorRT engine. This process involves several steps and we will look at them below :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837f05d",
   "metadata": {},
   "source": [
    "The generated ONNX file is saved in the [onnx_files](../onnx_files) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16873f1e-5691-4344-aac8-0bcdf3c316e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a2f6c",
   "metadata": {},
   "source": [
    "#### 1. Create Builder\n",
    "To create a builder, you must first create a logger. Then use the logger to create the builder.  Builder allows the creation of an optimized engine from a network definition. It allows the application to specify the maximum batch and workspace size, the minimum acceptable level of precision, timing iteration counts for autotuning, and an interface for quantizing networks to run in 8-bit precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3eb1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/01/2023-14:25:42] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a0636",
   "metadata": {},
   "source": [
    "#### 2. Create Network\n",
    "After the builder has been created, the first step in optimizing a model is to create a network definition. The EXPLICIT_BATCH flag is required in order to import models using the ONNX parser.  Network Definition provides methods for the application to specify the definition of a network. Input and output tensors can be specified, layers can be added, and there is an interface for configuring each supported layer type.\n",
    "Layers like convolutional and recurrent layers, and a Plugin layer type allows the application to implement functionality not natively supported by TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e99cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac991a",
   "metadata": {},
   "source": [
    "#### 3. Import model using ONNX Parser\n",
    "Now, the network definition must be populated from the ONNX representation. You can create an ONNX parser to populate the network as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c858b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "success = parser.parse_from_file(ONNX_FILE_PATH)\n",
    "for idx in range(parser.num_errors):\n",
    "    print(parser.get_error(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6343b",
   "metadata": {},
   "source": [
    "#### 4. Building an engine\n",
    "The next step is to create a build configuration specifying how TensorRT should optimize the model. This interface has many properties that you can set in order to control how TensorRT optimizes the network. \n",
    " Allows the application to execute inference. \n",
    "   - It supports synchronous and asynchronous execution, profiling, and enumeration and querying of the bindings for the engine inputs and outputs. \n",
    "   - A single-engine can have multiple execution contexts, allowing a single set of trained parameters to be used for the simultaneous execution of multiple batches.\n",
    "\n",
    "One important property is the maximum workspace size. Layer implementations often require a temporary workspace, and this parameter limits the maximum size that any layer in the network can use. If insufficient workspace is provided, it is possible that TensorRT will not be able to find an implementation for a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce55bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = builder.create_builder_config()\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 22) # 1 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2dc8f8",
   "metadata": {},
   "source": [
    "After the configuration has been specified, the engine can be built and serialized with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e8cebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/01/2023-14:12:46] [TRT] [W] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.4.1\n"
     ]
    }
   ],
   "source": [
    "serialized_engine = builder.build_serialized_network(network, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4243997",
   "metadata": {},
   "source": [
    "It may be useful to save the engine to a file for future use. You can do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d91e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../trt_engines/sample.engine\", \"wb\") as f:\n",
    "    f.write(serialized_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56c419",
   "metadata": {},
   "source": [
    "#### 5. Deserialize an Engine\n",
    "To perform inference, deserialize the engine using the Runtime interface. Like the builder, the runtime requires an instance of the logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "762874ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = trt.Runtime(TRT_LOGGER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b470f8e",
   "metadata": {},
   "source": [
    "First load the engine from a file. Then deserialize the engine from a memory buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78e0417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../trt_engines/sample.engine\", \"rb\") as f:\n",
    "    serialized_engine = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42324492",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = runtime.deserialize_cuda_engine(serialized_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c44d1",
   "metadata": {},
   "source": [
    "#### 6. Performing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd66913",
   "metadata": {},
   "source": [
    "The engine holds the optimized model, but to perform inference requires additional state for intermediate activations. An engine can have multiple execution contexts, allowing one set of weights to be used for multiple overlapping inference tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54a99caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/01/2023-14:14:10] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465dbb6",
   "metadata": {},
   "source": [
    "Allocate some host and device buffers for inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0279ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8757/2233168342.py:2: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  h_input = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=np.float32)\n",
      "/tmp/ipykernel_8757/2233168342.py:3: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=np.float32)\n"
     ]
    }
   ],
   "source": [
    "# Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) to hold host inputs/outputs.\n",
    "h_input = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=np.float32)\n",
    "h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=np.float32)\n",
    "# Allocate device memory for inputs and outputs.\n",
    "d_input = cuda.mem_alloc(h_input.nbytes)\n",
    "d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "# Create a stream in which to copy inputs/outputs and run inference.\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d339b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_input = np.array(preprocess_image(\"../data/turkish_coffee.jpg\").numpy(), dtype=np.float32, order='C')\n",
    "# Transfer input data to the GPU.\n",
    "cuda.memcpy_htod_async(d_input, host_input, stream)\n",
    "# Run inference.\n",
    "context. execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)\n",
    "# Transfer predictions back from the GPU.\n",
    "cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "# Synchronize the stream\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf881178",
   "metadata": {},
   "source": [
    "Create some space to store intermediate activation values. Since the engine holds the network definition and trained parameters, additional space is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4719b676-34a1-4107-95c3-338e619c99ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_data = torch.Tensor(h_output).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfa64093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: cup , confidence: 94.97858428955078 %, index: 968\n",
      "class: espresso , confidence: 3.9512522220611572 %, index: 967\n",
      "class: coffee mug , confidence: 0.6196940541267395 %, index: 504\n"
     ]
    }
   ],
   "source": [
    "postprocess(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4ef79",
   "metadata": {},
   "source": [
    "Finally we are able to recreate the same results that we obtained using pure pytorch model. In conclusion, we first converted a given model to it's ONNX representation, then used this ONNX representation to generate a TensorRT engine. Then, use this saved engine for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('bevfusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd2e805bf45cfce0ee59caeff624ebf2dd294551239d1fbf11f6751485705e2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
