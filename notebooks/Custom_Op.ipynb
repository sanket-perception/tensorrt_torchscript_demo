{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Operation in TorchScript\n",
    "In several situations, a network can have components that are unknown to TensorRT. \n",
    "\n",
    "In such cases, the network will fail to convert into TensorRT engine because of unknown ops.\n",
    "- There are several prevelant methods to integrate custom ops in a TensorRT engine. \n",
    "-  First, one can write custom plugins ( C++/ CUDA ) and register these plugins explicitly in TensorRTs plugin registry.\n",
    "- However, this method requires involved development and significant man-hours.\n",
    "- Another, efficient but not ideal method is to break up models into chunks and convert each chunk independently into TensorRT engines. Subsequently, when a part of the model fails to convert to TensorRT due to novel ops, we can convert these chunks to Torchscript instead. \n",
    "- In summary, every piece of the model compatible with TensorRT is converted directly, whil the remaining parts of the model are converted to torch script and stored as serialized objects.\n",
    "- Once all parts of the model are converted ( to either TRT or TorchScript), we stitch together these parts to give single output.\n",
    "- Outputs of one model chunk, is fed as input to the next chunk of the model. This is referred to as stitching engines.\n",
    "- Now, we will look at a simple example where a custom OP stored as torchscript earlier, is read and used in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to load a custom OPs called bev_pool_forward ( which is of course not a part of torch).\n",
    "As expected that would throw errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No such operator my_ops::bev_pool_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mmy_ops\u001b[39m.\u001b[39;49mbev_pool_forward\n",
      "File \u001b[0;32m~/.conda/envs/bevfusion/lib/python3.8/site-packages/torch/_ops.py:63\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m# Get the op `my_namespace::my_op` if available. This will also check\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# for overloads and raise an exception if there are more than one.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m qualified_op_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m::\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, op_name)\n\u001b[0;32m---> 63\u001b[0m op \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_jit_get_operation(qualified_op_name)\n\u001b[1;32m     64\u001b[0m \u001b[39m# let the script frontend know that op is identical to the builtin op\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# with qualified_op_name\u001b[39;00m\n\u001b[1;32m     66\u001b[0m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_builtins\u001b[39m.\u001b[39m_register_builtin(op, qualified_op_name)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No such operator my_ops::bev_pool_forward"
     ]
    }
   ],
   "source": [
    "torch.ops.my_ops.bev_pool_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the pre-compiled library as explained earlier, to register this operation in torch.\n",
    "This would allow us to use custom libraries inside torch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ops.load_library(\"/home/ubuntu/workstation/motorai/tensorrt_torchscript_demo/bev_pool/build/lib.linux-x86_64-cpython-38/bev_pool_forward.cpython-38-x86_64-linux-gnu.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._ops.my_ops.PyCapsule.bev_pool_forward>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ops.my_ops.bev_pool_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the pre-compiled torchscript and perform inference to demonstrate how a network with a custom op, can be loaded in Python/C++ environment \n",
    "for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev_pool_network = torch.jit.load(\"/home/ubuntu/workstation/motorai/tensorrt_torchscript_demo/torchscript_engines/bev_pool.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add our sample inputs to this network to get the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"../data/x.pt\")\n",
    "geom = torch.load(\"../data/geom.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bev_pool_network(geom,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 360, 360])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('bevfusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd2e805bf45cfce0ee59caeff624ebf2dd294551239d1fbf11f6751485705e2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
